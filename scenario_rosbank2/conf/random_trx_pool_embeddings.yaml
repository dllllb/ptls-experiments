defaults:
  - data_preprocessing: default
  - override hydra/sweeper: optuna
  - override hydra/sweeper/sampler: tpe
  - _self_

mode: ???

experiment_name: random_trx_pool_embeddings
tb_save_dir: lightning_logs/${experiment_name}/

hydra:
  job:
    chdir: true
  run:
    dir: outputs/${experiment_name}/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: multirun/${experiment_name}/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweeper:
    direction: maximize
    study_name: ${experiment_name}
    storage: null
    n_trials: 100
    n_jobs: 1
    params:
#      agg_model._args_.0.embeddings.mcc.in: tag(log, int(interval(2, 370)))
#      agg_model._args_.0.embeddings.channel_type.in: int(interval(2, 8))
#      agg_model._args_.0.embeddings.currency.in: tag(log, int(interval(2, 70)))
#      agg_model._args_.0.embeddings.trx_category.in: int(interval(2, 12))
      agg_model._args_.0.embeddings.mcc.out: tag(log, int(interval(2, 512)))
      agg_model._args_.0.embeddings.channel_type.out: tag(log, int(interval(2, 256)))
      agg_model._args_.0.embeddings.currency.out: tag(log, int(interval(2, 256)))
      agg_model._args_.0.embeddings.trx_category.out: tag(log, int(interval(2, 256)))
#      agg_model._args_.1.use_std: choice(false, true)
#      agg_model._args_.1.use_min: choice(false, true)
#      agg_model._args_.1.use_max: choice(false, true)

dataloader_inference:
  batch_size: 512
  num_workers: 4

run_model_partial:
  _target_: __main__.estimate_agg_embeddings
  _partial_: true

agg_model:
  _target_: torch.nn.Sequential
  _args_:
    - _target_: ptls.nn.TrxEncoder
      embeddings:
        mcc:
          in: 225
          out: 162
        channel_type:
          in: 3
          out: 32
        currency:
          in: 16
          out: 8
        trx_category:
          in: 11
          out: 93
      numeric_values:
        amount: log
      orthogonal: true
    - _target_: models.GlobalPoolingHead
      use_mean: true
      use_std: true
      use_min: true
      use_max: true

downstream_model:
  _target_: lightgbm.LGBMClassifier
  n_estimators: 500
  boosting_type: gbdt
  objective: binary
  metric: auc
  subsample: 0.5
  subsample_freq: 1
  learning_rate: 0.02
  feature_fraction: 0.75
  max_depth: 6
  lambda_l1: 1
  lambda_l2: 1
  min_data_in_leaf: 50
  random_state: 42
  n_jobs: 8
  reg_alpha: null
  reg_lambda: null
  colsample_bytree: null
  min_child_samples: null


#data_module:
#  valid_size: 0.1
#  valid_split_random_state: 42
#  max_seq_len: 1200
#  dm_params:
#    train_batch_size: 32
#    valid_batch_size: 256
#    train_num_workers: 8
#    valid_num_workers: 8
#    train_drop_last: True
#  augmentations:
#      - _target_: ptls.data_load.augmentations.RandomSlice
#        min_len: 250
#        max_len: 350
#        rate_for_min: 0.9
#      - _target_: ptls.data_load.augmentations.DropoutTrx
#        trx_dropout: 0.01
#
#trainer:
#  limit_train_batches: null
#  max_epochs: 10

#pl_module:
#  _target_: __main__.SequenceToTargetEx
#  seq_encoder:
#    _target_: ptls.nn.RnnSeqEncoder
#    trx_encoder:
#      _target_: ptls.nn.TrxEncoder
#      use_batch_norm_with_lens: true  # or not?
#      norm_embeddings: false
#      embeddings_noise: 0.0003
#      embeddings:
#        mcc:
#          in: 90  # 370
#          out: 12
#        channel_type:
#          in: 6  # 8
#          out: 12
#        currency:
#          in: 6  # 60
#          out: 30
#        trx_category:
#          in: 8  # 12
#          out: 30
#      numeric_values:
#        amount: log
#    hidden_size: 25
#    type: gru
#    bidir: false
#    trainable_starter: static
#  head:  # add ResNetBlocks
#    _target_: ptls.nn.Head
#    input_size: ${pl_module.seq_encoder.hidden_size}
#    use_batch_norm: true
#    objective: classification
#    num_classes: 1
#  loss:
#    _target_: ptls.loss.BCELoss
#  metric_list:
#    auroc:
#      _target_: torchmetrics.AUROC
#      num_classes: 2
#  optimizer_partial:
#    _partial_: true
#    _target_: torch.optim.Adam
#    lr: 0.0096
#    weight_decay: 0.0
#  lr_scheduler_partial:
#    _partial_: true
#    _target_: torch.optim.lr_scheduler.StepLR
#    step_size: 1
#    gamma: 0.44
#  weight_decay_trx_encoder: 0.0
