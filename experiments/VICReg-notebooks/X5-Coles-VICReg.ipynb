{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99929c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version (<2):  1.12.1+cu102\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "print('torch version (<2): ', torch.__version__)\n",
    "\n",
    "import ptls\n",
    "from ptls.frames import PtlsDataModule\n",
    "from ptls.frames.coles import losses, sampling_strategies\n",
    "from ptls.frames.coles import split_strategy\n",
    "from ptls.frames.inference_module import InferenceModule\n",
    "\n",
    "from ptls.nn.seq_encoder import agg_feature_seq_encoder\n",
    "from ptls.nn import RnnSeqEncoder, TrxEncoder, Head\n",
    "from ptls.nn.seq_encoder.agg_feature_seq_encoder import AggFeatureSeqEncoder\n",
    "\n",
    "from ptls.data_load.datasets import AugmentationDataset, MemoryMapDataset\n",
    "from ptls.data_load.augmentations import AllTimeShuffle, DropoutTrx\n",
    "from ptls.data_load.datasets.parquet_dataset import ParquetDataset\n",
    "from ptls.data_load.iterable_processing import SeqLenFilter, FeatureFilter\n",
    "from ptls.data_load.datasets import parquet_file_scan\n",
    "from ptls.data_load.datasets import ParquetDataset, ParquetFiles, AugmentationDataset\n",
    "from ptls.data_load.datasets import MemoryMapDataset\n",
    "from ptls.data_load.iterable_processing import SeqLenFilter, FeatureFilter\n",
    "from ptls.data_load.augmentations import DropoutTrx\n",
    "from ptls.data_load.datasets import inference_data_loader\n",
    "from ptls.data_load.utils import collate_feature_dict\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01c58a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('default')\n",
    "sns.set(rc={'figure.figsize':(8, 6)})\n",
    "sns.set_style('white')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "548e997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = PtlsDataModule(\n",
    "    train_data=ptls.frames.coles.ColesDataset(\n",
    "        splitter=split_strategy.SampleSlices(split_count=5, cnt_min=25, cnt_max=180),\n",
    "        data=ptls.data_load.datasets.AugmentationDataset(\n",
    "            data=MemoryMapDataset(\n",
    "                data=ParquetDataset(\n",
    "                    i_filters=[SeqLenFilter(min_seq_len=25)],\n",
    "                    data_files=parquet_file_scan(file_path='src/ptls-experiments/scenario_x5/data/train_trx.parquet',\n",
    "                                                 valid_rate=0.05,\n",
    "                                                 return_part='train')\n",
    "                )\n",
    "            \n",
    "            ),\n",
    "            f_augmentations=[ptls.data_load.augmentations.DropoutTrx(trx_dropout=0.01)]\n",
    "        )\n",
    "    ),\n",
    "    train_batch_size=256,\n",
    "    train_num_workers=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb845e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class L2NormEncoder(nn.Module):\n",
    "    def __init__(self, eps=1e-9):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x / (x.pow(2).sum(dim=-1, keepdim=True) + self.eps).pow(0.5)\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss\n",
    "\n",
    "    \"Signature verification using a siamese time delay neural network\", NIPS 1993\n",
    "    https://papers.nips.cc/paper/769-signature-verification-using-a-siamese-time-delay-neural-network.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin, sampling_strategy):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.pair_selector = sampling_strategy\n",
    "        \n",
    "        self.norm = L2NormEncoder()\n",
    "\n",
    "    def forward(self, embeddings, target):\n",
    "        embeddings = self.norm(embeddings)\n",
    "\n",
    "        positive_pairs, negative_pairs = self.pair_selector.get_pairs(embeddings, target)\n",
    "        positive_loss = F.pairwise_distance(embeddings[positive_pairs[:, 0]], embeddings[positive_pairs[:, 1]]).pow(2)\n",
    "        negative_loss = F.relu(\n",
    "            self.margin - F.pairwise_distance(embeddings[negative_pairs[:, 0]], embeddings[negative_pairs[:, 1]])\n",
    "        ).pow(2)\n",
    "        loss = torch.cat([positive_loss, negative_loss], dim=0)\n",
    "        \n",
    "        return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47fe6d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VICRegLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, ):\n",
    "        super(VICRegLoss, self).__init__()\n",
    "        \n",
    "        self._agg_encoder = AggFeatureSeqEncoder(         \n",
    "            embeddings={\n",
    "                \"level_3\": {\"in\": 200},\n",
    "                \"level_4\": {\"in\": 800},\n",
    "                \"segment_id\": {\"in\": 120},\n",
    "            },\n",
    "\n",
    "            numeric_values={\n",
    "                'trn_sum_from_iss': 'identity',\n",
    "                'netto': 'identity',\n",
    "                'regular_points_received': 'identity',            \n",
    "            },\n",
    "     \n",
    "            was_logified=True,  \n",
    "            log_scale_factor=1\n",
    "        )\n",
    "        self.norm = L2NormEncoder()\n",
    "        \n",
    "    def forward(self, embeddings, aggs):\n",
    "        aggs = self._agg_encoder(aggs)\n",
    "        aggs = self.norm(aggs.T)\n",
    "\n",
    "        cov_aggs_embs = (aggs @ embeddings) / len(embeddings)\n",
    "        cov_loss = cov_aggs_embs.pow_(2).sum()\n",
    "        \n",
    "        std_embeddings = torch.sqrt(embeddings.var(dim=0) + 0.0001)\n",
    "        std_loss = torch.mean(F.relu(1 - std_embeddings))\n",
    "        \n",
    "        return (cov_loss, std_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65a153c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "    def __init__(self, contrastiveLoss, vicregLoss):\n",
    "        super(Loss, self).__init__()\n",
    "        \n",
    "        self.contrastiveLoss = contrastiveLoss\n",
    "        self.vicregLoss = vicregLoss\n",
    "        \n",
    "    def forward(self, embeddings, target, aggs):\n",
    "        \n",
    "        (cov_loss, std_loss) = self.vicregLoss(embeddings, aggs)\n",
    "        con_loss = self.contrastiveLoss.forward(embeddings, target)\n",
    "        \n",
    "        return ((con_loss, cov_loss, std_loss), 0.55 * con_loss + 1 * cov_loss + 1 * std_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f004744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from ptls.data_load.padded_batch import PaddedBatch\n",
    "\n",
    "\n",
    "class ABSModule(pl.LightningModule):\n",
    "    @property\n",
    "    def metric_name(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @property\n",
    "    def is_requires_reduced_sequence(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def shared_step(self, x, y):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            x:\n",
    "            y:\n",
    "\n",
    "        Returns: y_h, y\n",
    "\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __init__(self, validation_metric=None,\n",
    "                       seq_encoder=None,\n",
    "                       loss=None,\n",
    "                       optimizer_partial=None,\n",
    "                       lr_scheduler_partial=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : dict\n",
    "            params for creating an encoder\n",
    "        seq_encoder : torch.nn.Module\n",
    "            sequence encoder, if not provided, will be constructed from params\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # self.save_hyperparameters()\n",
    "\n",
    "        self._loss = loss\n",
    "        self._seq_encoder = seq_encoder\n",
    "        self._seq_encoder.is_reduce_sequence = self.is_requires_reduced_sequence\n",
    "        self._validation_metric = validation_metric\n",
    "\n",
    "        self._optimizer_partial = optimizer_partial\n",
    "        self._lr_scheduler_partial = lr_scheduler_partial\n",
    "        \n",
    "    @property\n",
    "    def seq_encoder(self):\n",
    "        return self._seq_encoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._seq_encoder(x)\n",
    "\n",
    "    def training_step(self, batch, _):\n",
    "        \n",
    "        y_h, y = self.shared_step(*batch)\n",
    "                 \n",
    "        (con_loss, cov_loss, std_loss), loss = self._loss(y_h, y, batch[0])\n",
    "\n",
    "        self.log('con_loss', con_loss)\n",
    "        self.log('cov_loss', cov_loss)\n",
    "        self.log('std_loss', std_loss)\n",
    "        self.log('loss', loss)\n",
    "\n",
    "        if type(batch) is tuple:\n",
    "            x, y = batch\n",
    "            if isinstance(x, PaddedBatch):\n",
    "                self.log('seq_len', x.seq_lens.float().mean(), prog_bar=True)\n",
    "        else:\n",
    "            # this code should not be reached\n",
    "            self.log('seq_len', -1, prog_bar=True)\n",
    "            raise AssertionError('batch is not a tuple')\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, _):\n",
    "        y_h, y = self.shared_step(*batch)\n",
    "        self._validation_metric(y_h, y)\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        self.log(self.metric_name, self._validation_metric.compute(), prog_bar=True)\n",
    "        self._validation_metric.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = self._optimizer_partial(self.parameters())\n",
    "        scheduler = self._lr_scheduler_partial(optimizer)\n",
    "        \n",
    "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler = {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': self.metric_name,\n",
    "            }\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff6ab931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.frames.coles.metric import BatchRecallTopK\n",
    "from ptls.frames.coles.sampling_strategies import HardNegativePairSelector\n",
    "from ptls.nn.head import Head\n",
    "from ptls.nn.seq_encoder.containers import SeqEncoderContainer\n",
    "\n",
    "\n",
    "class CoLESModule(ABSModule):\n",
    "    def __init__(self,\n",
    "                 seq_encoder: SeqEncoderContainer = None,\n",
    "                 head=None,\n",
    "                 loss=None,\n",
    "                 validation_metric=None,\n",
    "                 optimizer_partial=None,\n",
    "                 lr_scheduler_partial=None):\n",
    "\n",
    "        if head is None:\n",
    "            head = Head(use_norm_encoder=True)\n",
    "\n",
    "        if loss is None:\n",
    "            loss = ContrastiveLoss(margin=0.5,\n",
    "                                   sampling_strategy=HardNegativePairSelector(neg_count=5))\n",
    "\n",
    "        if validation_metric is None:\n",
    "            validation_metric = BatchRecallTopK(K=4, metric='cosine')\n",
    "\n",
    "        super().__init__(validation_metric,\n",
    "                         seq_encoder,\n",
    "                         loss,\n",
    "                         optimizer_partial,\n",
    "                         lr_scheduler_partial\n",
    "                        )\n",
    "\n",
    "        self._head = head\n",
    "    @property\n",
    "    def metric_name(self):\n",
    "        return 'recall_top_k'\n",
    "\n",
    "    @property\n",
    "    def is_requires_reduced_sequence(self):\n",
    "        return True\n",
    "\n",
    "    def shared_step(self, x, y):\n",
    "        \n",
    "        y_h = self(x)\n",
    "        if self._head is not None:\n",
    "            y_h = self._head(y_h)\n",
    "        return y_h, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8245a37b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/jovyan/.local/share/virtualenvs/ptls-experiments-Xgdpvmv-/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:133: UserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\"You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type            | Params\n",
      "-------------------------------------------------------\n",
      "0 | _loss              | Loss            | 0     \n",
      "1 | _seq_encoder       | RnnSeqEncoder   | 886 K \n",
      "2 | _validation_metric | BatchRecallTopK | 0     \n",
      "3 | _head              | Head            | 0     \n",
      "-------------------------------------------------------\n",
      "886 K     Trainable params\n",
      "0         Non-trainable params\n",
      "886 K     Total params\n",
      "3.545     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e8e3bafe1f42839ad130ce477718f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = CoLESModule(\n",
    "      validation_metric=ptls.frames.coles.metric.BatchRecallTopK(K=4,\n",
    "                                                                 metric=\"cosine\"),\n",
    "      seq_encoder=RnnSeqEncoder(\n",
    "            trx_encoder=TrxEncoder(\n",
    "            use_batch_norm_with_lens=True,\n",
    "            norm_embeddings=False,\n",
    "            embeddings_noise=0.003,\n",
    "            \n",
    "            embeddings={\n",
    "                \"level_3\": {\"in\": 200, \"out\": 16},\n",
    "                \"level_4\": {\"in\": 800, \"out\": 16},\n",
    "                \"segment_id\": {\"in\": 120, \"out\": 16},\n",
    "            },\n",
    "\n",
    "            numeric_values={\n",
    "                \"trn_sum_from_iss\": \"identity\",\n",
    "                \"netto\": \"identity\",\n",
    "                \"regular_points_received\": \"identity\",\n",
    "            }\n",
    "                \n",
    "            ),\n",
    "            type=\"gru\",\n",
    "            hidden_size=800,\n",
    "            bidir=False,\n",
    "            trainable_starter=\"static\",\n",
    "      ),\n",
    "     \n",
    "      head=Head(\n",
    "            use_norm_encoder=False,\n",
    "            input_size=800,\n",
    "      ),\n",
    "        \n",
    "      loss=Loss(\n",
    "          ContrastiveLoss(\n",
    "            margin=0.5,\n",
    "            sampling_strategy=sampling_strategies.HardNegativePairSelector(neg_count=9),\n",
    "          ),\n",
    "          VICRegLoss()\n",
    "      ),\n",
    "    \n",
    "      optimizer_partial=partial(\n",
    "            torch.optim.Adam, \n",
    "            lr=0.002,\n",
    "            weight_decay=0.0\n",
    "      ),\n",
    "    \n",
    "      lr_scheduler_partial=partial(\n",
    "            torch.optim.lr_scheduler.StepLR,\n",
    "            step_size=3,\n",
    "            gamma=0.9025,\n",
    "      ),\n",
    ")\n",
    "\n",
    "logger = TensorBoardLogger('src/ptls-experiments/scenario_age_pred/lightning_logs',\n",
    "                           name='CoLES VICReg, hidden=800, coefs=0.55,1,1, margin=1')\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=logger,\n",
    "    num_sanity_val_steps=0,\n",
    "#     gpus=1,\n",
    "    accelerator=\"gpu\" ,\n",
    "#     auto_select_gpus=False,\n",
    "    max_epochs=30,\n",
    "    enable_checkpointing=False,\n",
    "    deterministic=True,\n",
    ")\n",
    "\n",
    "trainer.fit(model, data_module)\n",
    "print(trainer.logged_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8033f3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'src/ptls-experiments/scenario_x5/model-vicreg-new-coles-pretrained-512.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02c65e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('src/ptls-experiments/scenario_x5/model-vicreg-new-coles-pretrained-512.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1216dd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/share/virtualenvs/ptls-experiments-Xgdpvmv-/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:91: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/jovyan/.local/share/virtualenvs/ptls-experiments-Xgdpvmv-/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29fa0f648a5a4d24a5b8457aebed2bf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ptls.data_load.utils import collate_feature_dict\n",
    "from ptls.frames.inference_module import InferenceModule\n",
    "iterable_inference_dataset = ParquetDataset(\n",
    "    data_files=ParquetFiles(['src/ptls-experiments/scenario_x5/data/train_trx.parquet',\n",
    "                             'src/ptls-experiments/scenario_x5/data/test_trx.parquet'],                             \n",
    "                                                         \n",
    "                            ).data_files,\n",
    "    i_filters=[FeatureFilter(['client_id'])],\n",
    ")\n",
    "next(iter(iterable_inference_dataset))\n",
    "\n",
    "inference_dl = torch.utils.data.DataLoader(\n",
    "    dataset=iterable_inference_dataset,\n",
    "    collate_fn=collate_feature_dict,\n",
    "    shuffle=False,\n",
    "    batch_size=128,\n",
    "    num_workers=0,\n",
    ")\n",
    "next(iter(inference_dl)).payload\n",
    "\n",
    "mod = InferenceModule(model, pandas_output=True, model_out_name='emb')\n",
    "\n",
    "pred = pl.Trainer(gpus=1).predict(mod, inference_dl)\n",
    "\n",
    "embeddings_train_test = pd.concat(pred, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "503811ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_id</th>\n",
       "      <th>emb_0000</th>\n",
       "      <th>emb_0001</th>\n",
       "      <th>emb_0002</th>\n",
       "      <th>emb_0003</th>\n",
       "      <th>emb_0004</th>\n",
       "      <th>emb_0005</th>\n",
       "      <th>emb_0006</th>\n",
       "      <th>emb_0007</th>\n",
       "      <th>emb_0008</th>\n",
       "      <th>...</th>\n",
       "      <th>emb_0502</th>\n",
       "      <th>emb_0503</th>\n",
       "      <th>emb_0504</th>\n",
       "      <th>emb_0505</th>\n",
       "      <th>emb_0506</th>\n",
       "      <th>emb_0507</th>\n",
       "      <th>emb_0508</th>\n",
       "      <th>emb_0509</th>\n",
       "      <th>emb_0510</th>\n",
       "      <th>emb_0511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00037a9650</td>\n",
       "      <td>-0.010576</td>\n",
       "      <td>-0.002334</td>\n",
       "      <td>-0.009176</td>\n",
       "      <td>-0.023901</td>\n",
       "      <td>0.067811</td>\n",
       "      <td>-0.040258</td>\n",
       "      <td>-0.022467</td>\n",
       "      <td>0.032256</td>\n",
       "      <td>0.052724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016382</td>\n",
       "      <td>-0.041395</td>\n",
       "      <td>-0.020993</td>\n",
       "      <td>-0.012628</td>\n",
       "      <td>-0.008621</td>\n",
       "      <td>-0.034619</td>\n",
       "      <td>0.058045</td>\n",
       "      <td>0.041897</td>\n",
       "      <td>-0.000332</td>\n",
       "      <td>-0.000669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0005ce475d</td>\n",
       "      <td>0.014146</td>\n",
       "      <td>-0.043482</td>\n",
       "      <td>-0.006340</td>\n",
       "      <td>-0.001400</td>\n",
       "      <td>0.086435</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>-0.042507</td>\n",
       "      <td>0.039219</td>\n",
       "      <td>0.002245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>-0.051069</td>\n",
       "      <td>-0.021606</td>\n",
       "      <td>0.007528</td>\n",
       "      <td>-0.013405</td>\n",
       "      <td>-0.052815</td>\n",
       "      <td>0.080950</td>\n",
       "      <td>-0.011690</td>\n",
       "      <td>-0.005895</td>\n",
       "      <td>-0.008232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000c6e91c8</td>\n",
       "      <td>0.007199</td>\n",
       "      <td>0.005476</td>\n",
       "      <td>-0.005888</td>\n",
       "      <td>0.023478</td>\n",
       "      <td>0.085967</td>\n",
       "      <td>0.091710</td>\n",
       "      <td>-0.034176</td>\n",
       "      <td>0.032222</td>\n",
       "      <td>-0.032754</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014419</td>\n",
       "      <td>-0.045711</td>\n",
       "      <td>-0.011153</td>\n",
       "      <td>-0.031864</td>\n",
       "      <td>0.007012</td>\n",
       "      <td>-0.058724</td>\n",
       "      <td>0.046233</td>\n",
       "      <td>0.018627</td>\n",
       "      <td>-0.005846</td>\n",
       "      <td>-0.002067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00183b30d3</td>\n",
       "      <td>0.037918</td>\n",
       "      <td>-0.005334</td>\n",
       "      <td>-0.001674</td>\n",
       "      <td>-0.000382</td>\n",
       "      <td>0.040750</td>\n",
       "      <td>-0.014043</td>\n",
       "      <td>-0.013233</td>\n",
       "      <td>0.017424</td>\n",
       "      <td>-0.003058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010626</td>\n",
       "      <td>-0.029577</td>\n",
       "      <td>-0.010556</td>\n",
       "      <td>-0.004739</td>\n",
       "      <td>0.002030</td>\n",
       "      <td>-0.028003</td>\n",
       "      <td>0.001890</td>\n",
       "      <td>0.010655</td>\n",
       "      <td>-0.005744</td>\n",
       "      <td>0.004546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0036546652</td>\n",
       "      <td>0.004386</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>-0.005890</td>\n",
       "      <td>0.254002</td>\n",
       "      <td>0.040160</td>\n",
       "      <td>0.122409</td>\n",
       "      <td>-0.028133</td>\n",
       "      <td>-0.006420</td>\n",
       "      <td>-0.022485</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017634</td>\n",
       "      <td>-0.049006</td>\n",
       "      <td>-0.011241</td>\n",
       "      <td>-0.001659</td>\n",
       "      <td>-0.013277</td>\n",
       "      <td>-0.300536</td>\n",
       "      <td>0.087789</td>\n",
       "      <td>0.014445</td>\n",
       "      <td>-0.005073</td>\n",
       "      <td>-0.008319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ff4a662c34</td>\n",
       "      <td>0.010093</td>\n",
       "      <td>-0.008350</td>\n",
       "      <td>-0.004891</td>\n",
       "      <td>-0.008193</td>\n",
       "      <td>0.041547</td>\n",
       "      <td>-0.025976</td>\n",
       "      <td>-0.027283</td>\n",
       "      <td>0.035217</td>\n",
       "      <td>0.025977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>-0.033416</td>\n",
       "      <td>-0.008551</td>\n",
       "      <td>-0.003501</td>\n",
       "      <td>-0.000127</td>\n",
       "      <td>-0.028954</td>\n",
       "      <td>0.057420</td>\n",
       "      <td>-0.002152</td>\n",
       "      <td>-0.002127</td>\n",
       "      <td>-0.005268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ff6c9ba277</td>\n",
       "      <td>0.006813</td>\n",
       "      <td>0.003591</td>\n",
       "      <td>-0.006365</td>\n",
       "      <td>-0.033015</td>\n",
       "      <td>0.049616</td>\n",
       "      <td>-0.020507</td>\n",
       "      <td>-0.034578</td>\n",
       "      <td>0.068770</td>\n",
       "      <td>-0.112935</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016795</td>\n",
       "      <td>-0.048137</td>\n",
       "      <td>-0.040577</td>\n",
       "      <td>-0.023485</td>\n",
       "      <td>-0.000156</td>\n",
       "      <td>-0.046340</td>\n",
       "      <td>0.070769</td>\n",
       "      <td>0.033496</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>-0.001345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>ff90033a39</td>\n",
       "      <td>0.005974</td>\n",
       "      <td>-0.026335</td>\n",
       "      <td>-0.006777</td>\n",
       "      <td>-0.035574</td>\n",
       "      <td>0.052344</td>\n",
       "      <td>-0.026809</td>\n",
       "      <td>-0.022433</td>\n",
       "      <td>-0.002919</td>\n",
       "      <td>-0.070382</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020252</td>\n",
       "      <td>-0.054980</td>\n",
       "      <td>0.020380</td>\n",
       "      <td>-0.009821</td>\n",
       "      <td>-0.031865</td>\n",
       "      <td>-0.084448</td>\n",
       "      <td>0.060043</td>\n",
       "      <td>0.016959</td>\n",
       "      <td>-0.008727</td>\n",
       "      <td>-0.004619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ffbd796a83</td>\n",
       "      <td>0.017642</td>\n",
       "      <td>-0.006526</td>\n",
       "      <td>-0.002039</td>\n",
       "      <td>-0.003330</td>\n",
       "      <td>0.048242</td>\n",
       "      <td>-0.026711</td>\n",
       "      <td>-0.027495</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>-0.019942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010821</td>\n",
       "      <td>-0.033886</td>\n",
       "      <td>0.022008</td>\n",
       "      <td>-0.037607</td>\n",
       "      <td>0.007756</td>\n",
       "      <td>-0.046366</td>\n",
       "      <td>0.036116</td>\n",
       "      <td>0.023412</td>\n",
       "      <td>-0.003136</td>\n",
       "      <td>-0.005165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ffdaa0afbd</td>\n",
       "      <td>-0.015871</td>\n",
       "      <td>0.030817</td>\n",
       "      <td>-0.003329</td>\n",
       "      <td>-0.009284</td>\n",
       "      <td>0.021162</td>\n",
       "      <td>-0.028452</td>\n",
       "      <td>-0.051512</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>-0.195475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016259</td>\n",
       "      <td>-0.042987</td>\n",
       "      <td>-0.004751</td>\n",
       "      <td>0.020851</td>\n",
       "      <td>-0.003441</td>\n",
       "      <td>-0.011194</td>\n",
       "      <td>0.124324</td>\n",
       "      <td>0.002112</td>\n",
       "      <td>-0.007327</td>\n",
       "      <td>0.003660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400162 rows Ã— 513 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     client_id  emb_0000  emb_0001  emb_0002  emb_0003  emb_0004  emb_0005  \\\n",
       "0   00037a9650 -0.010576 -0.002334 -0.009176 -0.023901  0.067811 -0.040258   \n",
       "1   0005ce475d  0.014146 -0.043482 -0.006340 -0.001400  0.086435  0.000150   \n",
       "2   000c6e91c8  0.007199  0.005476 -0.005888  0.023478  0.085967  0.091710   \n",
       "3   00183b30d3  0.037918 -0.005334 -0.001674 -0.000382  0.040750 -0.014043   \n",
       "4   0036546652  0.004386  0.000443 -0.005890  0.254002  0.040160  0.122409   \n",
       "..         ...       ...       ...       ...       ...       ...       ...   \n",
       "29  ff4a662c34  0.010093 -0.008350 -0.004891 -0.008193  0.041547 -0.025976   \n",
       "30  ff6c9ba277  0.006813  0.003591 -0.006365 -0.033015  0.049616 -0.020507   \n",
       "31  ff90033a39  0.005974 -0.026335 -0.006777 -0.035574  0.052344 -0.026809   \n",
       "32  ffbd796a83  0.017642 -0.006526 -0.002039 -0.003330  0.048242 -0.026711   \n",
       "33  ffdaa0afbd -0.015871  0.030817 -0.003329 -0.009284  0.021162 -0.028452   \n",
       "\n",
       "    emb_0006  emb_0007  emb_0008  ...  emb_0502  emb_0503  emb_0504  emb_0505  \\\n",
       "0  -0.022467  0.032256  0.052724  ...  0.016382 -0.041395 -0.020993 -0.012628   \n",
       "1  -0.042507  0.039219  0.002245  ...  0.019000 -0.051069 -0.021606  0.007528   \n",
       "2  -0.034176  0.032222 -0.032754  ...  0.014419 -0.045711 -0.011153 -0.031864   \n",
       "3  -0.013233  0.017424 -0.003058  ...  0.010626 -0.029577 -0.010556 -0.004739   \n",
       "4  -0.028133 -0.006420 -0.022485  ...  0.017634 -0.049006 -0.011241 -0.001659   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "29 -0.027283  0.035217  0.025977  ...  0.011494 -0.033416 -0.008551 -0.003501   \n",
       "30 -0.034578  0.068770 -0.112935  ...  0.016795 -0.048137 -0.040577 -0.023485   \n",
       "31 -0.022433 -0.002919 -0.070382  ...  0.020252 -0.054980  0.020380 -0.009821   \n",
       "32 -0.027495  0.010101 -0.019942  ...  0.010821 -0.033886  0.022008 -0.037607   \n",
       "33 -0.051512  0.030700 -0.195475  ...  0.016259 -0.042987 -0.004751  0.020851   \n",
       "\n",
       "    emb_0506  emb_0507  emb_0508  emb_0509  emb_0510  emb_0511  \n",
       "0  -0.008621 -0.034619  0.058045  0.041897 -0.000332 -0.000669  \n",
       "1  -0.013405 -0.052815  0.080950 -0.011690 -0.005895 -0.008232  \n",
       "2   0.007012 -0.058724  0.046233  0.018627 -0.005846 -0.002067  \n",
       "3   0.002030 -0.028003  0.001890  0.010655 -0.005744  0.004546  \n",
       "4  -0.013277 -0.300536  0.087789  0.014445 -0.005073 -0.008319  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "29 -0.000127 -0.028954  0.057420 -0.002152 -0.002127 -0.005268  \n",
       "30 -0.000156 -0.046340  0.070769  0.033496  0.000864 -0.001345  \n",
       "31 -0.031865 -0.084448  0.060043  0.016959 -0.008727 -0.004619  \n",
       "32  0.007756 -0.046366  0.036116  0.023412 -0.003136 -0.005165  \n",
       "33 -0.003441 -0.011194  0.124324  0.002112 -0.007327  0.003660  \n",
       "\n",
       "[400162 rows x 513 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24bcfb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_train_test.to_pickle('src/ptls-experiments/scenario_x5/data/vicreg_coles_embeddings-512.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptls-experiments",
   "language": "python",
   "name": "ptls-experiments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
