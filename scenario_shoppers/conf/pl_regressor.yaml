ckpt: null
data_path: data
device: cuda
fsum: 0.683
qbin: 33
qlim: 0.995
test_fraction: 0
work_dir: lightning_logs/test

monte_carlo:
  agg_time: null
  benchmark:
    col: target_var
    dir: lightning_logs/benchmark
  chunk: 25000
  col: mcpred
  repeats: 100
  steps: 12

data_module:
  _target_: ptls.data_load.data_module.cls_data_module.ClsDataModuleTrain
  distribution_target_size: 0
  type: map
  setup:
    dataset_files:
      train_data_path: ${hydra:runtime.cwd}/${data_path}/train
    col_id: id
    col_id_dtype: int
    col_target: target_dist
    split_by: embeddings_validation
    fold_info: ${hydra:runtime.cwd}/${work_dir}/folds/folds.json
  train:
    min_seq_len: 0
    augmentations:
      -
        - RandomSlice
        -
          min_len: 400
          max_len: 800
      -
        - DropoutTrx
        -
          trx_dropout: 0.01
    num_workers: 16
    batch_size: 128
  valid:
    augmentations:
      -
        - SeqLenLimit
        -
          max_seq_len: 800
          strategy: tail
    num_workers: 16
    batch_size: 512

pl_module:
  _target_: ptls.frames.supervised.SequenceToTarget
  metric_list:
    acc:
      _target_: ptls.frames.supervised.metrics.BucketAccuracy
      scaler: null
    auc:
      _target_: ptls.frames.supervised.metrics.RankAUC
      scaler: null
    err:
      _target_: ptls.frames.supervised.metrics.UnivMeanError
      scaler: null
    jsd:
      _target_: ptls.frames.supervised.metrics.JSDiv
      scaler: null
  seq_encoder:
    _target_: ptls.nn.RnnSeqEncoder
    hidden_size: 128
    type: gru
    bidir: false
    trainable_starter: static
    trx_encoder:
      _target_: ptls.nn.TrxEncoder
      norm_embeddings: false
      embeddings_noise: 0.003
      embeddings:
        category:
          in: 111
          out: 64
      numeric_values:
        purchasequantity: identity
  head:
    _target_: ptls.nn.Head
    input_size: ${pl_module.seq_encoder.hidden_size}
    hidden_layers_sizes: ["${pl_module.seq_encoder.hidden_size}"]
    num_classes: 1
    objective: regression
    use_batch_norm: true
  loss:
    _target_: ptls.loss.ZILNLoss
  pretrained_lr: 0.01
  optimizer_partial:
    _partial_: true
    _target_: torch.optim.Adam
    lr: 0.01
    weight_decay: 0.0001
  lr_scheduler_partial:
    _partial_: true
    _target_: torch.optim.lr_scheduler.StepLR
    step_size: 1
    gamma: 0.87

trainer:
  accelerator: gpu
  accumulate_grad_batches: 1
  auto_lr_find: false
  auto_select_gpus: false
  deterministic: false
  devices: 1
  max_epochs: 20
  precision: 32

hydra:
  output_subdir: null
  run:
    dir: ${hydra:runtime.cwd}/${work_dir}
